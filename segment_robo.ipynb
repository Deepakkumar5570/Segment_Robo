{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)\n",
    "\n",
    "\n",
    "# !pip install -q 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "\n",
    "# !pip install -q jupyter_bbox_widget roboflow dataclasses-json supervision==0.23.0\n",
    "\n",
    "# !mkdir -p {HOME}/weights\n",
    "# !wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth -P {HOME}/weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(CHECKPOINT_PATH, \"; exist:\", os.path.isfile(CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p {HOME}/data\n",
    "\n",
    "# !wget -q https://media.roboflow.com/notebooks/examples/dog.jpeg -P {HOME}/data\n",
    "# !wget -q https://media.roboflow.com/notebooks/examples/dog-2.jpeg -P {HOME}/data\n",
    "# !wget -q https://media.roboflow.com/notebooks/examples/dog-3.jpeg -P {HOME}/data\n",
    "# !wget -q https://media.roboflow.com/notebooks/examples/dog-4.jpeg -P {HOME}/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "\n",
    "\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator, SamPredictor\n",
    "\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH).to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Automated Mask Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "# IMAGE_NAME = \"/home/stud1/Desktop/PlantDoc-Object-Detection-Dataset/TEST/1684.jpg\"\n",
    "IMAGE_NAME = \"/home/stud1/Desktop/data/Leaves_sunlight/2025-02-03_011/2025-02-03_011.png\"\n",
    "IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Maks With SAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "sam_result = mask_generator.generate(image_rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTPUT FORMAT\n",
    "\n",
    "\n",
    "SamAutomaticMaskGenerator returns a list of masks, where each mask is a dict containing various information about the mask:\n",
    "\n",
    "segmentation - [np.ndarray] - the mask with (W, H) shape, and bool type\n",
    "area - [int] - the area of the mask in pixels\n",
    "bbox - [List[int]] - the boundary box of the mask in xywh format\n",
    "predicted_iou - [float] - the model's own prediction for the quality of the mask\n",
    "point_coords - [List[List[float]]] - the sampled input point that generated this mask\n",
    "stability_score - [float] - an additional measure of mask quality\n",
    "crop_box - List[int] - the crop of the image used to generate this mask in xywh format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sam_result[0].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results visualisation with Supervision\n",
    "As of version 0.5.0 Supervision has native support for SAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "\n",
    "annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[image_bgr, annotated_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction with segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "masks = [\n",
    "    mask['segmentation']\n",
    "    for mask in sorted(sam_result, key=lambda x: x['area'], reverse=True)\n",
    "]\n",
    "\n",
    "# Ensure the grid can fit all images\n",
    "num_masks = len(masks)\n",
    "rows = 8\n",
    "cols = math.ceil(num_masks / rows)  # Use ceil to ensure all images fit\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(rows, cols),  # Adjusted grid size\n",
    "    size=(16, 16)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Automatci Grid Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "num_masks = len(masks)\n",
    "rows = math.ceil(math.sqrt(num_masks))  # Approximate square grid\n",
    "cols = math.ceil(num_masks / rows)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(rows, cols),\n",
    "    size=(16, 16)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imageio[freeimage]\n",
    "!pip install opencv-python-headless matplotlib imageio[freeimage] torch torchvision supervision segment-anything\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import imageio.v3 as iio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Import SAM libraries and supervision (for annotation)\n",
    "from segment_anything import sam_model_registry, SamAutomaticMaskGenerator\n",
    "import supervision as sv\n",
    "\n",
    "# ------------------ File Paths ------------------\n",
    "HDR_PATH = \"/home/stud1/Desktop/data/Leaves_sunlight/2025-02-03_011/capture/2025-02-03_011.hdr\"\n",
    "\n",
    "# ------------------ 1. Load & Normalize HDR Image ------------------\n",
    "hdr_image = None\n",
    "if os.path.exists(HDR_PATH):\n",
    "    try:\n",
    "        # Load HDR data using imageio\n",
    "        hdr_data = iio.imread(HDR_PATH)\n",
    "        # Normalize the spectral response to 0-255 for display\n",
    "        hdr_normalized = (hdr_data - np.min(hdr_data)) / (np.max(hdr_data) - np.min(hdr_data)) * 255\n",
    "        hdr_image = np.uint8(hdr_normalized)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading HDR:\", e)\n",
    "        hdr_image = None\n",
    "else:\n",
    "    print(\"HDR file not found at:\", HDR_PATH)\n",
    "\n",
    "# ------------------ 2. Prepare HDR Image for SAM ------------------\n",
    "if hdr_image is not None:\n",
    "    # SAM expects an RGB image; if your HDR is single-channel, convert it to 3-channel.\n",
    "    if len(hdr_image.shape) == 2 or hdr_image.shape[-1] == 1:\n",
    "        hdr_image_rgb = cv2.cvtColor(hdr_image, cv2.COLOR_GRAY2RGB)\n",
    "    else:\n",
    "        hdr_image_rgb = hdr_image.copy()\n",
    "else:\n",
    "    hdr_image_rgb = None\n",
    "\n",
    "# ------------------ 3. Load SAM Model & Generate Masks ------------------\n",
    "model_type = \"vit_h\"  # Using the \"vit_h\" variant\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if hdr_image_rgb is not None:\n",
    "    try:\n",
    "        # Initialize the SAM model without providing a checkpoint path explicitly.\n",
    "        sam = sam_model_registry[model_type]()\n",
    "        sam.to(device=device)\n",
    "\n",
    "        # Create the SAM automatic mask generator\n",
    "        mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "        # Generate segmentation masks for the HDR image\n",
    "        sam_results = mask_generator.generate(hdr_image_rgb)\n",
    "\n",
    "        if sam_results:\n",
    "            # Convert SAM output to a supervision detection object and annotate the image\n",
    "            detections = sv.Detections.from_sam(sam_result=sam_results)\n",
    "            mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "            annotated_hdr = mask_annotator.annotate(scene=hdr_image_rgb.copy(), detections=detections)\n",
    "        else:\n",
    "            print(\"SAM did not produce any masks for the HDR image.\")\n",
    "            annotated_hdr = hdr_image_rgb\n",
    "    except Exception as e:\n",
    "        print(\"Error during SAM segmentation:\", e)\n",
    "        annotated_hdr = hdr_image_rgb\n",
    "else:\n",
    "    annotated_hdr = None\n",
    "\n",
    "# ------------------ 4. Display the Segmented HDR Image ------------------\n",
    "if annotated_hdr is not None:\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(annotated_hdr)\n",
    "    plt.title(\"HDR Image with SAM Segmentation\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No HDR image available for segmentation/display.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Segmentes Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = \"/home/stud1/Desktop/PlantDoc-Object-Detection-Dataset/TEST\"\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(DATASET_PATH) if f.endswith(('.jpg', '.png'))]\n",
    "image_files = image_files[:5]  # Process only first 5 images\n",
    "\n",
    "for image_name in image_files:\n",
    "    image_path = os.path.join(DATASET_PATH, image_name)\n",
    "    \n",
    "    # Read and preprocess image\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Generate masks (assumes you have a valid mask_generator)\n",
    "    sam_result = mask_generator.generate(image_rgb)\n",
    "    \n",
    "    if not sam_result:\n",
    "        print(f\"No masks detected for {image_name}\")\n",
    "        continue\n",
    "    \n",
    "    # Extract the first (largest) mask -- full dictionary\n",
    "    largest_mask_data = max(sam_result, key=lambda x: x['area'])\n",
    "    \n",
    "    # Convert segmentation mask to displayable format (0 or 255)\n",
    "    segmentation_mask = largest_mask_data['segmentation'].astype(np.uint8) * 255\n",
    "    \n",
    "    # Annotate mask\n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    # Pass the entire dictionary to from_sam(), not just the mask\n",
    "    detections = sv.Detections.from_sam(sam_result=[largest_mask_data])\n",
    "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "    \n",
    "    # Display original and segmented image using Supervision\n",
    "    sv.plot_images_grid(\n",
    "        images=[image_bgr, annotated_image],\n",
    "        grid_size=(1, 2),\n",
    "        titles=['Source Image', 'Segmented Image']\n",
    "    )\n",
    "    \n",
    "    # Display the single mask directly with matplotlib to avoid axes.flat issues\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(segmentation_mask, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    plt.title('Largest Mask')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import supervision as sv\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = \"/home/stud1/Desktop/PlantDoc-Object-Detection-Dataset/TEST\"\n",
    "\n",
    "# Get list of image files\n",
    "image_files = [f for f in os.listdir(DATASET_PATH) if f.endswith(('.jpg', '.png'))]\n",
    "image_files = image_files[:5]  # Process only first 5 images\n",
    "\n",
    "for image_name in image_files:\n",
    "    image_path = os.path.join(DATASET_PATH, image_name)\n",
    "    \n",
    "    # Read and preprocess image\n",
    "    image_bgr = cv2.imread(image_path)\n",
    "    image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Generate masks\n",
    "    sam_result = mask_generator.generate(image_rgb)\n",
    "    \n",
    "    # Annotate masks\n",
    "    mask_annotator = sv.MaskAnnotator(color_lookup=sv.ColorLookup.INDEX)\n",
    "    detections = sv.Detections.from_sam(sam_result=sam_result)\n",
    "    annotated_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "    \n",
    "    # Display original and segmented image\n",
    "    sv.plot_images_grid(\n",
    "        images=[image_bgr, annotated_image],\n",
    "        grid_size=(1, 2),\n",
    "        titles=['Source Image', 'Segmented Image']\n",
    "    )\n",
    "    \n",
    "    # Extract masks sorted by area\n",
    "    masks = [mask['segmentation'] for mask in sorted(sam_result, key=lambda x: x['area'], reverse=True)]\n",
    "    \n",
    "    # Determine grid size\n",
    "    num_masks = len(masks)\n",
    "    rows = 8\n",
    "    cols = math.ceil(num_masks / rows)\n",
    "    \n",
    "    # Display masks\n",
    "    sv.plot_images_grid(\n",
    "        images=masks,\n",
    "        grid_size=(rows, cols),\n",
    "        size=(16, 16)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Segmentation with Bounding Box\n",
    "\n",
    "The SamPredictor class provides an easy interface to the model for prompting the model. It allows the user to first set an image using the set_image method, which calculates the necessary image embeddings. Then, prompts can be provided via the predict method to efficiently predict masks from those prompts. The model can take as input both point and box prompts, as well as masks from the previous iteration of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter_bbox_widget ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n",
    "!jupyter nbextension enable --py jupyter_bbox_widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# IMAGE_NAME = \"/home/stud1/Desktop/PlantDoc-Object-Detection-Dataset/TEST/07c.jpg\"\n",
    "# IMAGE_PATH = os.path.join(HOME, \"data\", IMAGE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # helper function that loads an image before adding it to the widget\n",
    "\n",
    "# import base64\n",
    "\n",
    "# def encode_image(filepath):\n",
    "#     with open(filepath, 'rb') as f:\n",
    "#         image_bytes = f.read()\n",
    "#     encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "#     return \"data:image/jpg;base64,\"+encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "\n",
    "# Ensure IMAGE_NAME is a string, not a tuple\n",
    "IMAGE_NAME = \"/home/stud1/Desktop/PlantDoc-Object-Detection-Dataset/TEST/1684.jpg\"\n",
    "\n",
    "# Define encode_image function correctly\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    return \"data:image/jpg;base64,\" + encoded \n",
    "\n",
    "# Use correct IMAGE_PATH (No need for os.path.join)\n",
    "IMAGE_PATH = IMAGE_NAME  \n",
    "\n",
    "# Create widget\n",
    "widget = BBoxWidget()\n",
    "widget.image = encode_image(IMAGE_PATH)\n",
    "widget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate masks with SAM\n",
    "\n",
    "NOTE: SamPredictor.predict method takes np.ndarray box argument in [x_min, y_min, x_max, y_max] format. Let's reorganise your data first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# default_box is going to be used if you will not draw any box on image above\n",
    "default_box = {'x': 168, 'y': 1247, 'width': 555, 'height': 678, 'label': ''}\n",
    "\n",
    "box = widget.bboxes[0] if widget.bboxes else default_box\n",
    "box = np.array([\n",
    "    box['x'],\n",
    "    box['y'],\n",
    "    box['x'] + box['width'],\n",
    "    box['y'] + box['height']\n",
    "])\n",
    "\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_predictor.set_image(image_rgb)\n",
    "\n",
    "masks, scores, logits = mask_predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results visualisation with Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
    "mask_annotator = sv.MaskAnnotator(color=sv.Color.RED, color_lookup=sv.ColorLookup.INDEX)\n",
    "\n",
    "detections = sv.Detections(\n",
    "    xyxy=sv.mask_to_xyxy(masks=masks),\n",
    "    mask=masks\n",
    ")\n",
    "detections = detections[detections.area == np.max(detections.area)]\n",
    "\n",
    "source_image = box_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "segmented_image = mask_annotator.annotate(scene=image_bgr.copy(), detections=detections)\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=[source_image, segmented_image],\n",
    "    grid_size=(1, 2),\n",
    "    titles=['source image', 'segmented image']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction with segmentation results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as v\n",
    "\n",
    "sv.plot_images_grid(\n",
    "    images=masks,\n",
    "    grid_size=(1, 4),\n",
    "    size=(16, 4)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
